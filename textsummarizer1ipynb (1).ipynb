{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvmB6RHSU0Dx",
        "outputId": "aa2680d0-d2c0-47e9-818f-0025a30c8dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
            "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
            "    self._complete_partial_requirements(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 489, in _complete_partial_requirements\n",
            "    self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/prepare.py\", line 614, in _prepare_linked_requirement\n",
            "    hashes.check_against_path(file_path)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/hashes.py\", line 102, in check_against_path\n",
            "    return self.check_against_file(file)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/hashes.py\", line 98, in check_against_file\n",
            "    return self.check_against_chunks(read_chunks(file))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/hashes.py\", line 82, in check_against_chunks\n",
            "    hash.update(chunk)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1536, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1634, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1644, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1706, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 978, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.11/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1230, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 953, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 119, in format\n",
            "    prefix += \" \" * get_indentation()\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 70, in get_indentation\n",
            "    return getattr(_log_state, \"indentation\", 0)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "nltk.download('brown')\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou5lc5InVpMH",
        "outputId": "b9bfbdb3-96e8-419a-8769-0d374edfbfc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "2qPIM1DtVrsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n"
      ],
      "metadata": {
        "id": "Qa_BtWuRVt5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence_embeddings(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    embeddings = (outputs.last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1).unsqueeze(-1)\n",
        "    return embeddings.cpu().detach().numpy().flatten()\n"
      ],
      "metadata": {
        "id": "XuwkavyNVwb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lsa_summarizer(text, num_sentences=10):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(text.split('. '))\n",
        "    svd = TruncatedSVD(n_components=num_sentences)\n",
        "    lsa = svd.fit_transform(X)\n",
        "    top_sentences = np.argsort(-lsa.sum(axis=1))[:num_sentences]\n",
        "    summary = '. '.join([text.split('. ')[i] for i in top_sentences])\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "Ef1Ch0qNVz3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "input_dim = 768  # Dimension of BERT embeddings\n",
        "hidden_dim = 256  # Dimension of the hidden layer\n",
        "autoencoder = Autoencoder(input_dim, hidden_dim).to(device)\n"
      ],
      "metadata": {
        "id": "71kRGF8ZV2tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from nltk.corpus import brown\n",
        "class TextSummarizationDataset(Dataset):\n",
        "    def __init__(self, texts):\n",
        "        self.texts = texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        embeddings = generate_sentence_embeddings(text)\n",
        "        return embeddings\n",
        "\n",
        "# Extract sentences from the Brown Corpus\n",
        "texts = brown.sents(categories='news')[:1000]  # Using 1000 sentences for demonstration\n",
        "texts = [' '.join(sent) for sent in texts]\n",
        "\n",
        "total_len = len(texts)\n",
        "train_len = int(0.75 * total_len)  # 75% for training\n",
        "val_len = total_len - train_len    # 25% for validation\n",
        "\n",
        "full_dataset = TextSummarizationDataset(texts)\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_len, val_len])\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "H5SxQDHQV6PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "epochs = 5  # Reduced from 20 to 5\n"
      ],
      "metadata": {
        "id": "IE-HmJZjV9mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    for batch in train_loader:\n",
        "        embeddings = batch.squeeze().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed = autoencoder(embeddings)\n",
        "        loss = criterion(reconstructed, embeddings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "E2Lg9Lz6WABA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class BERTEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Custom transformer to generate BERT embeddings.\"\"\"\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertModel.from_pretrained(model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Generate BERT embeddings for input text.\"\"\"\n",
        "        embeddings = []\n",
        "        for text in X:\n",
        "            inputs = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, truncation=True, padding='max_length')\n",
        "            inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
        "            embeddings.append(embedding)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "class LSASummarizerTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Custom transformer to generate LSA-based summaries.\"\"\"\n",
        "    def __init__(self, num_sentences=10):\n",
        "        self.num_sentences = num_sentences\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.lsa = TruncatedSVD(n_components=100)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the LSA model on the input text.\"\"\"\n",
        "        tfidf_matrix = self.vectorizer.fit_transform(X)\n",
        "        self.lsa.fit(tfidf_matrix)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Generate LSA-based summaries for input text.\"\"\"\n",
        "        tfidf_matrix = self.vectorizer.transform(X)\n",
        "        lsa_features = self.lsa.transform(tfidf_matrix)\n",
        "        return lsa_features"
      ],
      "metadata": {
        "id": "Hfv_gD_oWCi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract articles and their categories\n",
        "categories = brown.categories()\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for category in categories:\n",
        "    for fileid in brown.fileids(categories=category):\n",
        "        texts.append(' '.join(brown.words(fileid)))\n",
        "        labels.append(category)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.25, random_state=42)\n"
      ],
      "metadata": {
        "id": "e_HbdCERWFeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_text(text):\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "--MeTQuRbxPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_matrix(embeddings):\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "    return similarity_matrix\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uH43hB7ab1ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_units(similarity_matrix, units, num_units=10):\n",
        "    unit_ranks = np.argsort(-similarity_matrix.sum(axis=1))[:num_units]\n",
        "    ranked_units = [units[i] for i in unit_ranks]\n",
        "    return ranked_units\n"
      ],
      "metadata": {
        "id": "D-2lyPOCb763"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_performance(data_loader, model, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            embeddings = batch.squeeze().to(device)\n",
        "            reconstructed = model(embeddings)\n",
        "            loss = criterion(reconstructed, embeddings)\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    return avg_loss\n",
        "\n",
        "avg_loss = evaluate_model_performance(val_loader, autoencoder, criterion)\n",
        "print(\"Validation Loss:\", avg_loss)\n"
      ],
      "metadata": {
        "id": "UqiepWbNcE4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "xWvuW4j7mIab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_input_and_generate_summary():\n",
        "    # Step 1: Get input text\n",
        "    text = input(\"Enter the text to summarize: \")\n",
        "\n",
        "    # Step 2: Segment the text into sentences\n",
        "    sentences = segment_text(text)\n",
        "\n",
        "    # Step 3: Generate BERT embeddings for each sentence\n",
        "    bert_embeddings = np.array([generate_sentence_embeddings(sent) for sent in sentences])\n",
        "\n",
        "    # Step 4: Compute similarity matrix using BERT embeddings\n",
        "    similarity_matrix = compute_similarity_matrix(bert_embeddings)\n",
        "\n",
        "    # Step 5: Rank sentences based on similarity scores (BERT)\n",
        "    ranked_sentences = rank_units(similarity_matrix, sentences)\n",
        "\n",
        "    # Step 6: Generate BERT summary\n",
        "    bert_summary = '. '.join(ranked_sentences)\n",
        "\n",
        "    # Step 7: Generate LSA summary\n",
        "    lsa_summary = lsa_summarizer(text, num_sentences=10)\n",
        "    combined_summary = \"BERT Summary:\\n\" + '. '.join(ranked_sentences) + \"\\n\\nLSA Summary:\\n\" + lsa_summary\n",
        "\n",
        "    # Step 8: Print the combined summary\n",
        "    print(\"Combined Summary (BERT + LSA):\\n\", combined_summary)\n",
        "    # Step 9: Print BERT, LSA, and combined summaries\n",
        "    print(\"BERT Summary:\\n\", bert_summary)\n",
        "    print(\"\\nLSA Summary:\\n\", lsa_summary)\n",
        "\n",
        "# Example usage\n",
        "process_input_and_generate_summary()"
      ],
      "metadata": {
        "id": "XNHaVv34cMCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def segment_text(text):\n",
        "    \"\"\"Segment the input text into sentences.\"\"\"\n",
        "    sentences = text.split('. ')\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def generate_sentence_embeddings(sentence):\n",
        "    \"\"\"Generate sentence embeddings using a pre-trained BERT model.\"\"\"\n",
        "    # Placeholder for actual BERT embedding generation\n",
        "    # Replace with actual implementation (e.g., using HuggingFace Transformers)\n",
        "    return np.random.rand(768)  # Example: random vector of size 768\n",
        "\n",
        "def compute_similarity_matrix(embeddings):\n",
        "    \"\"\"Compute the similarity matrix using cosine similarity.\"\"\"\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    return cosine_similarity(embeddings)\n",
        "\n",
        "def rank_units(similarity_matrix, sentences):\n",
        "    \"\"\"Rank sentences based on similarity scores.\"\"\"\n",
        "    scores = np.mean(similarity_matrix, axis=1)\n",
        "    ranked_indices = np.argsort(scores)[::-1]  # Sort in descending order\n",
        "    return [sentences[i] for i in ranked_indices]\n",
        "\n",
        "def lsa_summarizer(text, num_sentences=10):\n",
        "    \"\"\"Generate an LSA-based summary.\"\"\"\n",
        "    # Placeholder for actual LSA summarization\n",
        "    # Replace with actual implementation (e.g., using Gensim or Scikit-learn)\n",
        "    sentences = segment_text(text)\n",
        "    return sentences[:num_sentences]  # Example: return first 10 sentences\n",
        "\n",
        "def combine_summaries(bert_sentences, lsa_sentences, max_sentences=10):\n",
        "    \"\"\"Combine BERT and LSA summaries into a single summary.\"\"\"\n",
        "    # Remove duplicates while preserving order\n",
        "    combined_sentences = []\n",
        "    seen = set()\n",
        "    for sent in bert_sentences + lsa_sentences:\n",
        "        if sent not in seen:\n",
        "            combined_sentences.append(sent)\n",
        "            seen.add(sent)\n",
        "    # Limit the number of sentences in the final summary\n",
        "    return '. '.join(combined_sentences[:max_sentences])\n",
        "\n",
        "def process_input_and_generate_summary():\n",
        "    \"\"\"Process input text and generate BERT, LSA, and combined summaries.\"\"\"\n",
        "    try:\n",
        "        # Step 1: Get input text\n",
        "        text = input(\"Enter the text to summarize: \")\n",
        "\n",
        "        # Step 2: Segment the text into sentences\n",
        "        sentences = segment_text(text)\n",
        "        if not sentences:\n",
        "            raise ValueError(\"No valid sentences found in the input text.\")\n",
        "\n",
        "        # Step 3: Generate BERT embeddings for each sentence\n",
        "        bert_embeddings = np.array([generate_sentence_embeddings(sent) for sent in sentences])\n",
        "\n",
        "        # Step 4: Compute similarity matrix using BERT embeddings\n",
        "        similarity_matrix = compute_similarity_matrix(bert_embeddings)\n",
        "\n",
        "        # Step 5: Rank sentences based on similarity scores (BERT)\n",
        "        bert_ranked_sentences = rank_units(similarity_matrix, sentences)\n",
        "\n",
        "        # Step 6: Generate BERT summary\n",
        "        bert_summary = '. '.join(bert_ranked_sentences)\n",
        "\n",
        "        # Step 7: Generate LSA summary\n",
        "        lsa_ranked_sentences = lsa_summarizer(text, num_sentences=10)\n",
        "        lsa_summary = '. '.join(lsa_ranked_sentences)\n",
        "\n",
        "        # Step 8: Combine BERT and LSA summaries\n",
        "        combined_summary = combine_summaries(bert_ranked_sentences, lsa_ranked_sentences)\n",
        "\n",
        "        # Step 9: Print the summaries\n",
        "        print(\"\\nBERT Summary:\\n\", bert_summary)\n",
        "        print(\"\\nLSA Summary:\\n\", lsa_summary)\n",
        "        print(\"\\nCombined Summary (BERT + LSA):\\n\", combined_summary)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage\n",
        "process_input_and_generate_summary()"
      ],
      "metadata": {
        "id": "bAUj1SojrVez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased').to(device)"
      ],
      "metadata": {
        "id": "4h6ZRog1fEyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the Pipeline\n",
        "vectorizer = TfidfVectorizer()\n",
        "lsa = TruncatedSVD(n_components=100)\n",
        "classifier = LogisticRegression()\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', vectorizer),\n",
        "    ('lsa', lsa),\n",
        "    ('clf', classifier)\n",
        "])\n"
      ],
      "metadata": {
        "id": "0iji6axZdIyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "def save_model(model, tokenizer, pipeline, model_name=\"bert_text_summarizer\", pipeline_name=\"lsa_classifier.pkl\", full_model_path=\"full_model.pth\"):\n",
        "    \"\"\"\n",
        "    Save the BERT model, tokenizer, and Pipeline object separately.\n",
        "\n",
        "    Args:\n",
        "        model: The BERT model (e.g., BertModel).\n",
        "        tokenizer: The BERT tokenizer (e.g., BertTokenizer).\n",
        "        pipeline: The scikit-learn Pipeline object.\n",
        "        model_name: Directory to save the BERT model and tokenizer.\n",
        "        pipeline_name: Filename to save the Pipeline object.\n",
        "        full_model_path: Filename to save the BERT model's state dictionary.\n",
        "    \"\"\"\n",
        "    # Save BERT model and tokenizer using Hugging Face's save_pretrained\n",
        "    model.save_pretrained(model_name)\n",
        "    tokenizer.save_pretrained(model_name)\n",
        "\n",
        "    # Save the Pipeline object using joblib\n",
        "    joblib.dump(pipeline, pipeline_name)\n",
        "\n",
        "    # Save the BERT model's state dictionary using torch.save\n",
        "    torch.save(model.state_dict(), full_model_path)\n",
        "\n",
        "    print(f\"BERT model and tokenizer saved in directory: {model_name}/\")\n",
        "    print(f\"Pipeline saved as: {pipeline_name}\")\n",
        "    print(f\"BERT model state dictionary saved as: {full_model_path}\")"
      ],
      "metadata": {
        "id": "liJA6T04ej6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9LR-iVthYjB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}